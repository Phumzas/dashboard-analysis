{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit transformers pyngrok pandas matplotlib wordcloud fpdf\n",
        "\n"
      ],
      "metadata": {
        "id": "_Rs7rhXGE9sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df618027-455d-431e-88e0-4206dbc4e5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace YOUR_NGROK_AUTH_TOKEN_HERE with your token from https://dashboard.ngrok.com/get-started/setup\n",
        "!ngrok authtoken 2yM0h8JyfroDcDanXONETBnGyRJ_68a9CuDk2ATsZ4cTUijM7\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zhyljbQHZSi",
        "outputId": "76ca63c9-7db5-45f1-f87e-95250eccae4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from fpdf import FPDF\n",
        "import io\n",
        "\n",
        "st.set_page_config(page_title=\"Sentiment Analysis Dashboard\", layout=\"wide\")\n",
        "\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def load_sentiment_model():\n",
        "    return pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "def extract_keywords(text, n=10):\n",
        "    words = text.lower().split()\n",
        "    freq = {}\n",
        "    for w in words:\n",
        "        if len(w) > 3 and w.isalpha():\n",
        "            freq[w] = freq.get(w, 0) + 1\n",
        "    keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [k for k, v in keywords[:n]]\n",
        "\n",
        "def plot_sentiment_distribution(df):\n",
        "    counts = df['label'].value_counts()\n",
        "    fig, ax = plt.subplots()\n",
        "    counts.plot(kind='bar', ax=ax, color=['green', 'orange', 'red'])\n",
        "    ax.set_title(\"Sentiment Distribution\")\n",
        "    ax.set_xlabel(\"Sentiment\")\n",
        "    ax.set_ylabel(\"Number of Texts\")\n",
        "    st.pyplot(fig)\n",
        "\n",
        "def plot_wordcloud(keywords):\n",
        "    text = \" \".join(keywords)\n",
        "    wc = WordCloud(width=400, height=200, background_color='white').generate(text)\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    st.pyplot(plt)\n",
        "\n",
        "def export_results(df):\n",
        "    st.markdown(\"### Export Results\")\n",
        "\n",
        "    # CSV\n",
        "    csv = df.to_csv(index=False).encode('utf-8')\n",
        "    st.download_button(\"Download CSV\", data=csv, file_name='sentiment_results.csv')\n",
        "\n",
        "    # JSON\n",
        "    json_data = df.to_json(orient='records')\n",
        "    st.download_button(\"Download JSON\", data=json_data, file_name='sentiment_results.json')\n",
        "\n",
        "    # PDF\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.cell(200, 10, txt=\"Sentiment Analysis Results\", ln=True, align='C')\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        pdf.ln(5)\n",
        "        pdf.cell(0, 10, txt=f\"Text: {row['text'][:70]}...\", ln=True)\n",
        "        pdf.cell(0, 10, txt=f\"Sentiment: {row['label']} (Confidence: {row['score']:.2f})\", ln=True)\n",
        "        pdf.cell(0, 10, txt=f\"Keywords: {', '.join(row['keywords'])}\", ln=True)\n",
        "\n",
        "    pdf_bytes = pdf.output(dest='S').encode('latin-1')\n",
        "    st.download_button(\"Download PDF\", data=pdf_bytes, file_name=\"sentiment_results.pdf\")\n",
        "\n",
        "\n",
        "def explain_sentiment(text, label):\n",
        "    keywords = extract_keywords(text)\n",
        "    reasons = \", \".join(keywords[:3])\n",
        "    return f\"This text was labeled **{label}** mainly due to keywords like: {reasons}\"\n",
        "\n",
        "def main():\n",
        "    st.title(\"ðŸ“Š Sentiment Analysis Dashboard\")\n",
        "    st.markdown(\"Analyze text sentiment using Hugging Face Transformers!\")\n",
        "\n",
        "    model = load_sentiment_model()\n",
        "\n",
        "    st.markdown(\"## Enter text or upload a text file (.txt, .csv)\")\n",
        "    input_method = st.radio(\"Select input method\", (\"Text input\", \"File upload\"))\n",
        "\n",
        "    texts = []\n",
        "    if input_method == \"Text input\":\n",
        "        user_text = st.text_area(\"Enter text to analyze\", height=150)\n",
        "        if user_text:\n",
        "            texts = [user_text]\n",
        "    else:\n",
        "        uploaded_file = st.file_uploader(\"Upload a .txt or .csv file\", type=['txt', 'csv'])\n",
        "        if uploaded_file:\n",
        "            if uploaded_file.type == \"text/csv\":\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "                if 'text' in df.columns:\n",
        "                    texts = df['text'].dropna().tolist()\n",
        "                else:\n",
        "                    st.error(\"CSV must contain a 'text' column.\")\n",
        "            else:\n",
        "                file_text = uploaded_file.read().decode('utf-8').strip()\n",
        "                texts = [line for line in file_text.split('\\n') if line.strip()]\n",
        "\n",
        "    if texts:\n",
        "        st.markdown(f\"### Analyzing {len(texts)} texts...\")\n",
        "        results = []\n",
        "        for text in texts:\n",
        "            try:\n",
        "                output = model(text)[0]\n",
        "                label = output['label']\n",
        "                score = output['score']\n",
        "                keywords = extract_keywords(text)\n",
        "                explanation = explain_sentiment(text, label)\n",
        "                results.append({\n",
        "                    'text': text,\n",
        "                    'label': label,\n",
        "                    'score': score,\n",
        "                    'keywords': keywords,\n",
        "                    'explanation': explanation\n",
        "                })\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error analyzing text: {e}\")\n",
        "\n",
        "        df_results = pd.DataFrame(results)\n",
        "        st.dataframe(df_results[['text', 'label', 'score', 'keywords', 'explanation']])\n",
        "\n",
        "        plot_sentiment_distribution(df_results)\n",
        "\n",
        "        all_keywords = sum(df_results['keywords'].tolist(), [])\n",
        "        st.markdown(\"### Keywords WordCloud\")\n",
        "        plot_wordcloud(all_keywords)\n",
        "\n",
        "        export_results(df_results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQJWHoveHijp",
        "outputId": "90c1d691-df53-4e8e-e8ed-c69158fa7a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "get_ipython().system_raw('streamlit run app.py &')\n",
        "\n",
        "# Wait for the server to spin up\n",
        "time.sleep(5)\n",
        "\n",
        "# Connect ngrok tunnel to port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"ðŸš€ Your Streamlit app is live at: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd4o7nOOHofJ",
        "outputId": "7b8a3b9e-4101-4cb3-9504-09202188f3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Your Streamlit app is live at: NgrokTunnel: \"https://7c4e-34-53-87-72.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}